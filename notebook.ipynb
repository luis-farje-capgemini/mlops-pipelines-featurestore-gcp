{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLOps using Kubeflow and Feature Store\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before start\n",
    "- Google Cloud Storage Bucket / Pipelines / Feature Store must be created on same region {suggested: europe-west3(Frankfurt)}\n",
    "- Pipelines considerations when failing if jobs is \"in progress\", needs to improve adding code until previous import is done\n",
    "- Some issues with $tfx 2.7 so needed to downgrade to 2.5\n",
    "- Align on aiplatform sdk\n",
    "- Have good mlops use case for using feature store serving (costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Create new virtual environment\n",
    "conda create -n myenv python=3.7\n",
    "conda activate myenv\n",
    "\n",
    "#Launch Jupyter notebook using teh virtual environment\n",
    "pip install --user ipykernel\n",
    "python -m ipykernel install --user --name=myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.google.client import AIPlatformClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud.aiplatform_v1 import (\n",
    "    FeaturestoreOnlineServingServiceClient,\n",
    "    FeaturestoreServiceClient,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n",
    "from google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\n",
    "from google.cloud.aiplatform_v1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    featurestore_online_service as featurestore_online_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import (\n",
    "    featurestore_service as featurestore_service_pb2,\n",
    ")\n",
    "from google.cloud.aiplatform_v1.types import io as io_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import trainer_component\n",
    "from src.generator import generator_component\n",
    "from src.ingester import ingester_component\n",
    "from src import load_component\n",
    "import importlib\n",
    "from src import feature_store_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket, BigQuery and Feature Store Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://\" + PROJECT_ID + \"movielens\"\n",
    "REGION = \"europe-west4\"\n",
    "\n",
    "API_ENDPOINT = \"europe-west4-aiplatform.googleapis.com\"\n",
    "INPUT_CSV_FILE = \"\"\n",
    "FEATURESTORE_ID = \"movie_prediction\"\n",
    "FEATURE_STORE_REGION = REGION\n",
    "ENTITY_TYPE_ID=\"movie_entity\"\n",
    "ENTITY_ID_FIELD=\"user_id\"\n",
    "\n",
    "# BigQuery parameters\n",
    "BIGQUERY_DATASET_ID = f\"{PROJECT_ID}.movielens_dataset\"\n",
    "BIGQUERY_LOCATION = \"EU\"\n",
    "BIGQUERY_TABLE_ID = f\"{BIGQUERY_DATASET_ID}.training_dataset\"\n",
    "BIGQUERY_RAW_TABLE_ID = f\"{BIGQUERY_DATASET_ID}.raw_dataset\"\n",
    "BIGQUERY_INPUT_URI=f\"bq://{BIGQUERY_RAW_TABLE_ID}\"\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/prajitdatta/movielens-100k-dataset\n",
    "# Dataset parameters\n",
    "RAW_DATA_PATH = BUCKET_NAME+\"/raw_data/u.data\"\n",
    "\n",
    "# u.data -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "# Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The data is randomly ordered. \n",
    "# This is a tab separated list of user id | item id | rating | timestamp.\n",
    "# The time stamps are unix seconds since 1/1/1970 UTC\n",
    "\n",
    "# Pipeline parameters\n",
    "PIPELINE_NAME = \"movie-prediction\"\n",
    "ENABLE_CACHING = False\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline\"\n",
    "PIPELINE_SPEC_PATH = \"metadata_pipeline.json\"\n",
    "OUTPUT_COMPONENT_SPEC = \"output-component.yaml\"\n",
    "\n",
    "BIGQUERY_TMP_FILE = (\n",
    "    \"tmp.json\" \n",
    ")\n",
    "BIGQUERY_MAX_ROWS = 5 \n",
    "\n",
    "TFRECORD_FILE = (\n",
    "    f\"{BUCKET_NAME}/trainer_input_path/*\"  \n",
    ")\n",
    "\n",
    "LOGGER_PUBSUB_TOPIC = \"logger-pubsub-topic\"\n",
    "LOGGER_CLOUD_FUNCTION = \"logger-cloud-function\"\n",
    "\n",
    "# Trainer parameters\n",
    "TRAINING_ARTIFACTS_DIR = f\"{BUCKET_NAME}/artifacts\"\n",
    "TRAINING_REPLICA_COUNT = 1\n",
    "TRAINING_MACHINE_TYPE = \"n1-standard-4\"\n",
    "TRAINING_ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
    "TRAINING_ACCELERATOR_COUNT = 0\n",
    "\n",
    "# Deployer parameters\n",
    "TRAINED_POLICY_DISPLAY_NAME = \"movielens-trained-policy\"\n",
    "ENDPOINT_DISPLAY_NAME = \"movielens-endpoint\"\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "ENDPOINT_REPLICA_COUNT = 1  \n",
    "ENDPOINT_ACCELERATOR_TYPE = \"ACCELERATOR_TYPE_UNSPECIFIED\"  \n",
    "ENDPOINT_ACCELERATOR_COUNT = 0 \n",
    "\n",
    "# Prediction container parameters\n",
    "PREDICTION_CONTAINER = \"prediction-container\"\n",
    "PREDICTION_CONTAINER_DIR = \"src/prediction_container\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77e81ca774b11d7c56e2cca515643be85a0de53eafc7203808e7c65d1c7cb7e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('3.8.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
